{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOKXX8RBBVtsOujgoGN8ZS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":12,"metadata":{"id":"KOCLCAOkDNxI","colab":{"base_uri":"https://localhost:8080/","height":584},"executionInfo":{"status":"ok","timestamp":1753556941556,"user_tz":-180,"elapsed":39096,"user":{"displayName":"- Alfeim","userId":"00719047510581572332"}},"outputId":"b77e1966-6da2-4478-e369-09cd100dabaa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","\n","--- Loading Training Data for Label Information ---\n","\n","--- Preprocessing Labels ---\n","Discovered 7 unique labels: ['A', 'B', 'C', 'D', 'E', 'F', 'Z']\n","Label processing complete.\n","\n","--- Analyzing Label Co-occurrence ---\n","Found 14 strong label co-occurrence patterns\n","\n","--- Preparing for Test Set Evaluation ---\n","Searching for checkpoints in '/content/drive/MyDrive/AraHealthQA/MentalQA/Task1/output/improved_camelbert_checkpoints'...\n","Found latest checkpoint to use: /content/drive/MyDrive/AraHealthQA/MentalQA/Task1/output/improved_camelbert_checkpoints/checkpoint-330\n","\n","--- Starting Evaluation on Test Set ---\n","Loading model and tokenizer from: /content/drive/MyDrive/AraHealthQA/MentalQA/Task1/output/improved_camelbert_checkpoints/checkpoint-330\n","Instantiating custom model structure...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([7, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([7]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Loading weights from: /content/drive/MyDrive/AraHealthQA/MentalQA/Task1/output/improved_camelbert_checkpoints/checkpoint-330/model.safetensors\n","Loading tokenizer from original base model: 'CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment'\n","Loading test data from: /content/drive/MyDrive/AraHealthQA/MentalQA/Task1/subtask1_input_test.tsv\n","Loaded 150 samples from the test set.\n","Tokenizing test data...\n","Generating predictions on the test set...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Generated 150 predictions for the test set.\n","Test set predictions successfully saved to '/content/drive/MyDrive/AraHealthQA/MentalQA/Task1/output/predictions_on_test_set.tsv'\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","IMPROVED Multi-Label Arabic Mental Health Classification Model\n","- MODIFIED: Can now run in \"evaluation-only\" mode to skip training and\n","  predict directly on a test set using an existing checkpoint.\n","\"\"\"\n","import os\n","import pandas as pd\n","from tqdm.auto import tqdm\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import glob\n","\n","# Import Hugging Face Transformers components\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.metrics import f1_score, classification_report\n","\n","# --- Configuration ---\n","MODEL_NAME = \"CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment\"\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {DEVICE}\")\n","\n","# =================================================================================\n","# --- START OF MODE CONFIGURATION ---\n","# Set to True to skip training and only run evaluation on the test set.\n","# Set to False to run the full training process first.\n","EVALUATE_ONLY = True\n","# =================================================================================\n","# --- END OF MODE CONFIGURATION ---\n","# =================================================================================\n","\n","# --- File Paths ---\n","# Training data paths (still needed in EVALUATE_ONLY mode to understand the labels)\n","DATA_PATH = '/content/drive/MyDrive/AraHealthQA/MentalQA/Task1/dev_data.tsv'\n","LABELS_PATH = '/content/drive/MyDrive/AraHealthQA/MentalQA/Task1/train_label.tsv'\n","# Directory where your saved checkpoints are located\n","TRAINING_OUTPUT_DIR = '/content/drive/MyDrive/AraHealthQA/MentalQA/Task1/output/improved_camelbert_checkpoints'\n","\n","# Test data paths\n","TEST_DATA_PATH = '/content/drive/MyDrive/AraHealthQA/MentalQA/Task1/subtask1_input_test.tsv'\n","TEST_PREDICTION_OUTPUT_PATH = '/content/drive/MyDrive/AraHealthQA/MentalQA/Task1/output/predictions_on_test_set.tsv'\n","\n","\n","# --- Custom Model with Focal Loss ---\n","class ImprovedMultiLabelModel(nn.Module):\n","    # (This class is unchanged)\n","    def __init__(self, model_name, num_labels, alpha=1.0, gamma=2.0):\n","        super().__init__()\n","        self.bert = AutoModelForSequenceClassification.from_pretrained(\n","            model_name, num_labels=num_labels, problem_type=\"multi_label_classification\", ignore_mismatched_sizes=True\n","        )\n","        self.alpha, self.gamma, self.num_labels = alpha, gamma, num_labels\n","    def focal_loss(self, logits, labels):\n","        BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(logits, labels)\n","        pt = torch.exp(-BCE_loss)\n","        return (self.alpha * (1-pt)**self.gamma * BCE_loss).mean()\n","    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n","        outputs = self.bert.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        sequence_output = outputs.last_hidden_state\n","        pooled_output = sequence_output[:, 0]\n","        logits = self.bert.classifier(pooled_output)\n","        loss = None\n","        if labels is not None:\n","            loss = self.focal_loss(logits, labels)\n","        return SequenceClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n","\n","# --- Helper Functions ---\n","def robust_read_lines(file_path):\n","    # (This function is unchanged)\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        return [line.strip() for line in f.readlines()]\n","\n","def load_and_prepare_data(data_path, labels_path):\n","    # (This function is unchanged)\n","    questions, labels = robust_read_lines(data_path), robust_read_lines(labels_path)\n","    if len(questions) != len(labels):\n","        raise ValueError(f\"Mismatch in line count between data and labels.\")\n","    return pd.DataFrame({'text': questions, 'labels_str': labels})\n","\n","def process_label_strings(label_series):\n","    # (This function is unchanged)\n","    processed_labels = []\n","    for s in label_series:\n","        labels = [label.strip() for label in s.split(',') if label.strip()]\n","        processed_labels.append(labels)\n","    return processed_labels\n","\n","def analyze_label_cooccurrence(labels_matrix, label_names):\n","    # (This function is unchanged)\n","    cooccurrence = np.dot(labels_matrix.T, labels_matrix)\n","    label_frequencies = np.sum(labels_matrix, axis=0)\n","    cooccurrence_prob = {}\n","    for i, label1 in enumerate(label_names):\n","        for j, label2 in enumerate(label_names):\n","            if i != j and label_frequencies[i] > 0:\n","                prob = cooccurrence[i, j] / label_frequencies[i]\n","                if prob > 0.3:\n","                    cooccurrence_prob[(label1, label2)] = prob\n","    return cooccurrence_prob\n","\n","class ImprovedMentalQADataset(Dataset):\n","    # (This function is unchanged)\n","    def __init__(self, encodings, labels):\n","        self.encodings, self.labels = encodings, labels\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n","        return item\n","    def __len__(self):\n","        return len(self.labels)\n","\n","def adaptive_threshold_prediction(logits, label_names, cooccurrence_prob, base_threshold=0.3):\n","    # (This function is unchanged)\n","    probs = 1 / (1 + np.exp(-logits))\n","    predictions = []\n","    for i in range(len(probs)):\n","        sample_probs = probs[i]\n","        predicted_labels = {label_names[idx] for idx in np.where(sample_probs >= base_threshold)[0]}\n","        for label in list(predicted_labels):\n","            for idx, other_label in enumerate(label_names):\n","                if other_label not in predicted_labels and (label, other_label) in cooccurrence_prob:\n","                    cooccur_prob = cooccurrence_prob[(label, other_label)]\n","                    adjusted_threshold = base_threshold * (1 - cooccur_prob * 0.5)\n","                    if sample_probs[idx] >= adjusted_threshold:\n","                        predicted_labels.add(other_label)\n","        if not predicted_labels:\n","            predicted_labels.add(label_names[np.argmax(sample_probs)])\n","        if len(predicted_labels) > 4:\n","            label_prob_pairs = sorted([(label, sample_probs[label_names.index(label)]) for label in predicted_labels], key=lambda x: x[1], reverse=True)\n","            predicted_labels = {pair[0] for pair in label_prob_pairs[:4]}\n","        predictions.append(sorted(list(predicted_labels)))\n","    return predictions\n","\n","\n","\n","def adaptive_threshold_prediction(logits, label_names, cooccurrence_prob, base_threshold=0.3):\n","    # Convert the numpy array of label names to a list to enable the .index() method\n","    label_names_list = list(label_names)\n","\n","    probs = 1 / (1 + np.exp(-logits))\n","    predictions = []\n","\n","    for i in range(len(probs)):\n","        sample_probs = probs[i]\n","        # Use the list for indexing\n","        predicted_labels = {label_names_list[idx] for idx in np.where(sample_probs >= base_threshold)[0]}\n","\n","        for label in list(predicted_labels):\n","            # Use the list for enumerating\n","            for idx, other_label in enumerate(label_names_list):\n","                if other_label not in predicted_labels and (label, other_label) in cooccurrence_prob:\n","                    cooccur_prob = cooccurrence_prob[(label, other_label)]\n","                    adjusted_threshold = base_threshold * (1 - cooccur_prob * 0.5)\n","                    if sample_probs[idx] >= adjusted_threshold:\n","                        predicted_labels.add(other_label)\n","\n","        if not predicted_labels:\n","            # Use the list for indexing\n","            predicted_labels.add(label_names_list[np.argmax(sample_probs)])\n","\n","        if len(predicted_labels) > 4:\n","            # This is the line that caused the error, now fixed by using the list.\n","            label_prob_pairs = sorted([(label, sample_probs[label_names_list.index(label)]) for label in predicted_labels], key=lambda x: x[1], reverse=True)\n","            predicted_labels = {pair[0] for pair in label_prob_pairs[:4]}\n","\n","        predictions.append(sorted(list(predicted_labels)))\n","\n","    return predictions\n","\n","\n","\n","# --- Main Execution ---\n","def main():\n","    # This part is common to both modes: We must load the training data to learn\n","    # the label set (for the MultiLabelBinarizer) and co-occurrence probabilities.\n","    print(\"\\n--- Loading Training Data for Label Information ---\")\n","    full_df = load_and_prepare_data(DATA_PATH, LABELS_PATH)\n","    if full_df is None: return\n","\n","    print(\"\\n--- Preprocessing Labels ---\")\n","    all_labels_flat = [label for sublist in process_label_strings(full_df['labels_str']) for label in sublist]\n","    all_labels = sorted(list(set(all_labels_flat)))\n","    print(f\"Discovered {len(all_labels)} unique labels: {all_labels}\")\n","    mlb = MultiLabelBinarizer(classes=all_labels)\n","    train_labels_for_fitting = mlb.fit_transform(process_label_strings(full_df['labels_str']))\n","    print(\"Label processing complete.\")\n","\n","    print(\"\\n--- Analyzing Label Co-occurrence ---\")\n","    cooccurrence_prob = analyze_label_cooccurrence(train_labels_for_fitting, all_labels)\n","    print(f\"Found {len(cooccurrence_prob)} strong label co-occurrence patterns\")\n","\n","    if not EVALUATE_ONLY:\n","        # --- TRAINING MODE ---\n","        print(\"\\n--- RUNNING IN TRAINING MODE ---\")\n","        print(\"Tokenizing training text...\")\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","        train_encodings = tokenizer(full_df['text'].tolist(), truncation=True, padding=True, max_length=256)\n","        train_dataset = ImprovedMentalQADataset(train_encodings, train_labels_for_fitting)\n","\n","        print(\"Initializing model for training...\")\n","        model = ImprovedMultiLabelModel(MODEL_NAME, len(all_labels), alpha=1.0, gamma=2.0).to(DEVICE)\n","        training_args = TrainingArguments(\n","            output_dir=TRAINING_OUTPUT_DIR, num_train_epochs=15, per_device_train_batch_size=8,\n","            gradient_accumulation_steps=2, learning_rate=2e-5, warmup_steps=100,\n","            weight_decay=0.01, logging_dir='./logs', logging_steps=20, save_strategy=\"epoch\",\n","            save_total_limit=3, dataloader_num_workers=2, fp16=True if torch.cuda.is_available() else False,\n","        )\n","        trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset)\n","\n","        print(\"Starting fine-tuning...\")\n","        trainer.train()\n","        print(\"Fine-tuning complete.\")\n","\n","    # --- EVALUATION ON TEST SET (runs in both modes) ---\n","    print(\"\\n--- Preparing for Test Set Evaluation ---\")\n","    # Find the latest checkpoint from the training output directory\n","    print(f\"Searching for checkpoints in '{TRAINING_OUTPUT_DIR}'...\")\n","    checkpoints = sorted(glob.glob(os.path.join(TRAINING_OUTPUT_DIR, 'checkpoint-*')), key=os.path.getmtime)\n","\n","    if not checkpoints:\n","        print(f\"FATAL: No checkpoints found in '{TRAINING_OUTPUT_DIR}'. Cannot run evaluation.\")\n","        return\n","\n","    latest_checkpoint_path = checkpoints[-1]\n","    print(f\"Found latest checkpoint to use: {latest_checkpoint_path}\")\n","\n","    # Call the evaluation function\n","    evaluate_on_test_set(\n","        checkpoint_path=latest_checkpoint_path,\n","        test_data_path=TEST_DATA_PATH,\n","        output_path=TEST_PREDICTION_OUTPUT_PATH,\n","        mlb=mlb,\n","        cooccurrence_prob=cooccurrence_prob\n","    )\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GQtK312EEk4","executionInfo":{"status":"ok","timestamp":1753555725855,"user_tz":-180,"elapsed":23033,"user":{"displayName":"- Alfeim","userId":"00719047510581572332"}},"outputId":"736cab9c-58fa-4896-d16f-b1821fe8a3b1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from safetensors.torch import load_file"],"metadata":{"id":"xpJu80nlHxzh","executionInfo":{"status":"ok","timestamp":1753556656663,"user_tz":-180,"elapsed":11,"user":{"displayName":"- Alfeim","userId":"00719047510581572332"}}},"execution_count":9,"outputs":[]}]}