{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kuj9-HTYYxWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "k_folds_mentalqa_arabert_optimized.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/19NClhPis--SpLjxllBNlOiQoWl22l77w\n",
        "\"\"\"\n",
        "\n",
        "# Mount Google Drive to access your files\n",
        "# This is necessary when running in Google Colab.\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    print(\"Not running in Google Colab. Skipping drive mount.\")\n",
        "\n",
        "\n",
        "# =================================================================================\n",
        "# Cell 1: Common Imports and Setup\n",
        "# =================================================================================\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "import glob\n",
        "from torch.utils.data import Dataset\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "# Import Hugging Face Transformers components\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "\n",
        "# Import scikit-learn components\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, classification_report, jaccard_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# =================================================================================\n",
        "# Cell 2: Configuration with NEW Optimized Parameters\n",
        "# =================================================================================\n",
        "\n",
        "# --- Model and Device Configuration ---\n",
        "# MODIFIED: Set to the specified AraBERT model\n",
        "MODEL_NAME = \"aubmindlab/bert-base-arabertv2\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using model: {MODEL_NAME}\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# --- File Paths Configuration (Update if your structure is different) ---\n",
        "BASE_DRIVE_DIR = '/content/drive/MyDrive/AraHealthQA/MentalQA/Task1/'\n",
        "DATA_PATH = os.path.join(BASE_DRIVE_DIR, 'dev_data.tsv')\n",
        "LABELS_PATH = os.path.join(BASE_DRIVE_DIR, 'train_label.tsv')\n",
        "TEST_DATA_PATH = os.path.join(BASE_DRIVE_DIR, 'data/subtask1_input_test.tsv')\n",
        "TEST_LABELS_PATH = os.path.join(BASE_DRIVE_DIR, 'data/subtask1_output_test.tsv')\n",
        "\n",
        "# MODIFIED: Output directory changed to reflect the new model and its \"optimized\" status\n",
        "TRAINING_OUTPUT_DIR_BASE = os.path.join(BASE_DRIVE_DIR, 'output/arabert_optimized_kfold')\n",
        "RESULTS_DIR = os.path.join(BASE_DRIVE_DIR, 'results')\n",
        "\n",
        "# --- K-Fold and Hyperparameter Configuration ---\n",
        "N_SPLITS = 5 # Number of folds for cross-validation\n",
        "\n",
        "# MODIFIED: Using the new best parameters from your Optuna trial\n",
        "OPTIMIZED_PARAMS = {\n",
        "    'learning_rate': 5.273957732715589e-05,\n",
        "    'num_train_epochs': 13,\n",
        "    'weight_decay': 0.04131058607286182,\n",
        "    'focal_alpha': 0.9702303056621574,\n",
        "    'focal_gamma': 1.39543909126709,\n",
        "    'base_threshold': 0.20408644287720523\n",
        "}\n",
        "print(\"\\n--- Using Optimized Hyperparameters ---\")\n",
        "for key, value in OPTIMIZED_PARAMS.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "print(\"-------------------------------------\\n\")\n",
        "\n",
        "\n",
        "# =================================================================================\n",
        "# Cell 3: Custom Model, Datasets, and Helper Functions\n",
        "# =================================================================================\n",
        "\n",
        "# --- Custom Model with Focal Loss ---\n",
        "class FocalLossMultiLabelModel(nn.Module):\n",
        "    def __init__(self, model_name, num_labels, alpha, gamma):\n",
        "        super().__init__()\n",
        "        # ignore_mismatched_sizes=True re-initializes the classification head\n",
        "        # for the new number of labels, which is correct for fine-tuning.\n",
        "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_labels,\n",
        "            problem_type=\"multi_label_classification\",\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "        self.alpha, self.gamma = alpha, gamma\n",
        "\n",
        "    def focal_loss(self, logits, labels):\n",
        "        # BCEWithLogitsLoss is numerically stable\n",
        "        BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(logits, labels)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        # The core focal loss formula\n",
        "        return (self.alpha * (1 - pt)**self.gamma * BCE_loss).mean()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
        "        # Standard BERT forward pass for sequence classification\n",
        "        outputs = self.bert.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0] # Use the [CLS] token's representation\n",
        "        logits = self.bert.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # Calculate focal loss if labels are provided (during training)\n",
        "            loss = self.focal_loss(logits, labels.float())\n",
        "\n",
        "        return SequenceClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n",
        "\n",
        "# --- Simplified Model for Inference ---\n",
        "class InferenceModel(nn.Module):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_labels,\n",
        "            problem_type=\"multi_label_classification\",\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "    # For inference, we only need the logits, so the forward pass is simpler\n",
        "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
        "        return self.bert(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
        "\n",
        "# --- Data Handling Functions ---\n",
        "def robust_read_lines(file_path):\n",
        "    \"\"\"Reads lines from a file, stripping whitespace, with UTF-8 encoding.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return [line.strip() for line in f.readlines()]\n",
        "\n",
        "def load_and_prepare_data(data_path, labels_path=None):\n",
        "    \"\"\"Loads questions and optional labels into a pandas DataFrame.\"\"\"\n",
        "    questions = robust_read_lines(data_path)\n",
        "    df_data = {'text': questions}\n",
        "    if labels_path:\n",
        "        labels = robust_read_lines(labels_path)\n",
        "        if len(questions) != len(labels):\n",
        "            raise ValueError(f\"Mismatch in line count between {data_path} and {labels_path}.\")\n",
        "        df_data['labels_str'] = labels\n",
        "    return pd.DataFrame(df_data)\n",
        "\n",
        "def process_label_strings(label_series):\n",
        "    \"\"\"Converts comma-separated label strings into a list of lists.\"\"\"\n",
        "    return [[label.strip() for label in str(s).split(',') if label.strip()] for s in label_series]\n",
        "\n",
        "# --- Post-Processing and Analysis Functions ---\n",
        "def analyze_label_cooccurrence(labels_matrix, label_names, min_cooccurrence_prob=0.3):\n",
        "    \"\"\"Calculates a dictionary of co-occurrence probabilities between labels.\"\"\"\n",
        "    cooccurrence_matrix = np.dot(labels_matrix.T.astype(float), labels_matrix.astype(float))\n",
        "    label_frequencies = np.sum(labels_matrix, axis=0)\n",
        "    cooccurrence_prob = {}\n",
        "    for i, label1 in enumerate(label_names):\n",
        "        for j, label2 in enumerate(label_names):\n",
        "            if i != j and label_frequencies[i] > 0:\n",
        "                prob = cooccurrence_matrix[i, j] / label_frequencies[i]\n",
        "                if prob > min_cooccurrence_prob:\n",
        "                    cooccurrence_prob[(label1, label2)] = prob\n",
        "    return cooccurrence_prob\n",
        "\n",
        "def adaptive_threshold_prediction(logits, label_names, cooccurrence_prob, base_threshold, max_preds=4):\n",
        "    \"\"\"Generates predictions using an adaptive threshold based on label co-occurrence.\"\"\"\n",
        "    probs = 1 / (1 + np.exp(-logits)) # Sigmoid function to get probabilities\n",
        "    predictions = []\n",
        "    for i in range(len(probs)):\n",
        "        sample_probs = probs[i]\n",
        "        # Initial prediction based on the base threshold\n",
        "        predicted_labels = {label_names[idx] for idx in np.where(sample_probs >= base_threshold)[0]}\n",
        "\n",
        "        # Dynamically add labels based on co-occurrence probabilities\n",
        "        for label in list(predicted_labels):\n",
        "            for idx, other_label in enumerate(label_names):\n",
        "                if other_label not in predicted_labels and (label, other_label) in cooccurrence_prob:\n",
        "                    cooccur_prob = cooccurrence_prob[(label, other_label)]\n",
        "                    # Lower the threshold for labels that are likely to co-occur\n",
        "                    adjusted_threshold = base_threshold * (1 - cooccur_prob * 0.5)\n",
        "                    if sample_probs[idx] >= adjusted_threshold:\n",
        "                        predicted_labels.add(other_label)\n",
        "\n",
        "        # Ensure at least one label is predicted for every sample\n",
        "        if not predicted_labels:\n",
        "            predicted_labels.add(label_names[np.argmax(sample_probs)])\n",
        "\n",
        "        # Enforce a maximum number of predictions\n",
        "        if len(predicted_labels) > max_preds:\n",
        "            label_prob_pairs = sorted([(l, sample_probs[label_names.index(l)]) for l in predicted_labels], key=lambda x: x[1], reverse=True)\n",
        "            predicted_labels = {p[0] for p in label_prob_pairs[:max_preds]}\n",
        "\n",
        "        predictions.append(sorted(list(predicted_labels)))\n",
        "    return predictions\n",
        "\n",
        "# --- PyTorch Dataset Class ---\n",
        "class MentalQADataset(Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "# --- Utility Function ---\n",
        "def find_best_checkpoint(fold_dir):\n",
        "    \"\"\"Finds the path to the best checkpoint in a fold directory.\"\"\"\n",
        "    checkpoint_dirs = glob.glob(os.path.join(fold_dir, 'checkpoint-*'))\n",
        "    if not checkpoint_dirs:\n",
        "        raise FileNotFoundError(f\"No checkpoint directory found in {fold_dir}\")\n",
        "    # The 'best' model is the one saved last when load_best_model_at_end=True\n",
        "    return max(checkpoint_dirs, key=os.path.getmtime)\n",
        "\n",
        "\n",
        "# =================================================================================\n",
        "# Cell 4: Main Training Function\n",
        "# =================================================================================\n",
        "def main_training():\n",
        "    \"\"\"\n",
        "    Performs K-fold cross-validation training using the specified model and hyperparameters.\n",
        "    \"\"\"\n",
        "    print(\"========================================\")\n",
        "    print(\"         STARTING TRAINING RUN          \")\n",
        "    print(\"========================================\")\n",
        "\n",
        "    # 1. Create Output Directories\n",
        "    os.makedirs(os.path.dirname(TRAINING_OUTPUT_DIR_BASE), exist_ok=True)\n",
        "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "    # 2. Load and Prepare Data\n",
        "    print(\"\\n--- Loading Data from Google Drive ---\")\n",
        "    full_df = load_and_prepare_data(DATA_PATH, LABELS_PATH)\n",
        "    full_df = full_df.reset_index(drop=True)\n",
        "\n",
        "    # 3. Preprocess Labels\n",
        "    print(\"\\n--- Preprocessing Labels ---\")\n",
        "    all_labels_nested = process_label_strings(full_df['labels_str'])\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    mlb.fit(all_labels_nested)\n",
        "    all_labels = list(mlb.classes_)\n",
        "    print(f\"Discovered {len(all_labels)} unique labels: {all_labels}\")\n",
        "\n",
        "    # 4. K-Fold Cross-Validation Setup\n",
        "    kfold = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "    oof_preds, oof_true, oof_indices = [], [], []\n",
        "\n",
        "    # 5. Iterate Through Folds\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(full_df)):\n",
        "        print(f\"\\n===== Fold {fold+1}/{N_SPLITS} =====\")\n",
        "\n",
        "        fold_output_dir = f\"{TRAINING_OUTPUT_DIR_BASE}_fold_{fold+1}\"\n",
        "        if os.path.exists(fold_output_dir):\n",
        "            print(f\"Removing existing directory: {fold_output_dir}\")\n",
        "            shutil.rmtree(fold_output_dir)\n",
        "\n",
        "        train_df, val_df = full_df.iloc[train_idx], full_df.iloc[val_idx]\n",
        "        print(f\"Training on {len(train_df)} samples, Validating on {len(val_df)} samples.\")\n",
        "\n",
        "        train_labels = mlb.transform(process_label_strings(train_df['labels_str']))\n",
        "        val_labels = mlb.transform(process_label_strings(val_df['labels_str']))\n",
        "\n",
        "        cooccurrence_prob = analyze_label_cooccurrence(train_labels, all_labels)\n",
        "        print(f\"Found {len(cooccurrence_prob)} strong label co-occurrence patterns for this fold.\")\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, max_length=256)\n",
        "        val_encodings = tokenizer(val_df['text'].tolist(), truncation=True, padding=True, max_length=256)\n",
        "\n",
        "        train_dataset = MentalQADataset(train_encodings, train_labels)\n",
        "        val_dataset = MentalQADataset(val_encodings, val_labels)\n",
        "\n",
        "        def compute_metrics(p):\n",
        "            \"\"\"Custom metric computation function for the Trainer.\"\"\"\n",
        "            logits, labels = p.predictions, p.label_ids\n",
        "            predicted_labels_list = adaptive_threshold_prediction(\n",
        "                logits, all_labels, cooccurrence_prob, base_threshold=OPTIMIZED_PARAMS['base_threshold']\n",
        "            )\n",
        "            y_pred = mlb.transform(predicted_labels_list)\n",
        "            y_true = labels.astype(int)\n",
        "            return {'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0)}\n",
        "\n",
        "        print(\"\\n--- Initializing New Model for Fold ---\")\n",
        "        model = FocalLossMultiLabelModel(\n",
        "            MODEL_NAME,\n",
        "            num_labels=len(all_labels),\n",
        "            alpha=OPTIMIZED_PARAMS['focal_alpha'],\n",
        "            gamma=OPTIMIZED_PARAMS['focal_gamma']\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=fold_output_dir,\n",
        "            num_train_epochs=OPTIMIZED_PARAMS['num_train_epochs'],\n",
        "            learning_rate=OPTIMIZED_PARAMS['learning_rate'],\n",
        "            weight_decay=OPTIMIZED_PARAMS['weight_decay'],\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=16,\n",
        "            warmup_steps=50,\n",
        "            logging_strategy=\"epoch\",\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"f1_weighted\",\n",
        "            greater_is_better=True,\n",
        "            save_total_limit=1,\n",
        "            fp16=True if torch.cuda.is_available() else False,\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            compute_metrics=compute_metrics\n",
        "        )\n",
        "\n",
        "        print(f\"\\n--- Starting Fine-Tuning for Fold {fold+1} ---\")\n",
        "        trainer.train()\n",
        "\n",
        "        print(\"\\n--- Generating Predictions on Validation Set for Fold ---\")\n",
        "        predictions = trainer.predict(val_dataset)\n",
        "        predicted_labels_list = adaptive_threshold_prediction(\n",
        "            predictions.predictions, all_labels, cooccurrence_prob, base_threshold=OPTIMIZED_PARAMS['base_threshold']\n",
        "        )\n",
        "        oof_preds.extend(predicted_labels_list)\n",
        "        oof_true.extend(val_df['labels_str'].tolist())\n",
        "        oof_indices.extend(val_idx)\n",
        "\n",
        "    # 6. Final Out-of-Fold (OOF) Evaluation\n",
        "    print(\"\\n\\n===== Overall K-Fold Performance Analysis (OOF) =====\")\n",
        "    # Ensure predictions are in the original order\n",
        "    order = np.argsort(oof_indices)\n",
        "    ordered_preds = np.array(oof_preds, dtype=object)[order]\n",
        "    ordered_true_str = np.array(oof_true, dtype=object)[order]\n",
        "\n",
        "    y_true_final = mlb.transform(process_label_strings(pd.Series(ordered_true_str)))\n",
        "    y_pred_final = mlb.transform(ordered_preds)\n",
        "\n",
        "    f1_weighted_overall = f1_score(y_true_final, y_pred_final, average='weighted', zero_division=0)\n",
        "    print(f\"\\nOverall Weighted F1 Score across all folds: {f1_weighted_overall:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Overall Per-Label Performance (based on OOF predictions) ---\")\n",
        "    print(classification_report(y_true_final, y_pred_final, target_names=all_labels, zero_division=0))\n",
        "    print(\"‚úÖ Training complete.\")\n",
        "\n",
        "\n",
        "# =================================================================================\n",
        "# Cell 5: Main Evaluation Function\n",
        "# =================================================================================\n",
        "def evaluate_kfold_ensemble():\n",
        "    \"\"\"\n",
        "    Loads all k-fold models, gets averaged predictions (ensembling),\n",
        "    and evaluates the final performance on the test set.\n",
        "    \"\"\"\n",
        "    print(\"\\n\\n========================================\")\n",
        "    print(\"        STARTING EVALUATION RUN         \")\n",
        "    print(\"========================================\")\n",
        "\n",
        "    # 1. Load Tokenizer, Test Data, and Full Training Data for context\n",
        "    print(\"\\n--- 1. Loading tokenizer and datasets ---\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    test_df = load_and_prepare_data(TEST_DATA_PATH, TEST_LABELS_PATH)\n",
        "    full_train_df = load_and_prepare_data(DATA_PATH, LABELS_PATH)\n",
        "\n",
        "    # 2. Re-create Label Binarizer and Co-occurrence Map from FULL training data\n",
        "    print(\"\\n--- 2. Preprocessing labels for evaluation context ---\")\n",
        "    all_labels_nested = process_label_strings(full_train_df['labels_str'])\n",
        "    mlb = MultiLabelBinarizer().fit(all_labels_nested)\n",
        "    all_labels = list(mlb.classes_)\n",
        "    train_labels_binary = mlb.transform(all_labels_nested)\n",
        "    cooccurrence_prob = analyze_label_cooccurrence(train_labels_binary, all_labels)\n",
        "    print(f\"Built co-occurrence map from {len(full_train_df)} training samples.\")\n",
        "    NUM_LABELS = len(all_labels)\n",
        "\n",
        "    # 3. Tokenize Test Data\n",
        "    print(\"\\n--- 3. Tokenizing the test set ---\")\n",
        "    test_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True, max_length=256)\n",
        "    test_dataset = MentalQADataset(test_encodings)\n",
        "\n",
        "    # 4. Perform Ensemble Prediction by Averaging Logits\n",
        "    print(f\"\\n--- 4. Generating predictions from {N_SPLITS} ensembled models ---\")\n",
        "    all_logits = []\n",
        "    for i in range(N_SPLITS):\n",
        "        fold = i + 1\n",
        "        try:\n",
        "            fold_dir = f\"{TRAINING_OUTPUT_DIR_BASE}_fold_{fold}\"\n",
        "            checkpoint_dir = find_best_checkpoint(fold_dir)\n",
        "            weights_path = os.path.join(checkpoint_dir, 'model.safetensors')\n",
        "            if not os.path.exists(weights_path):\n",
        "                weights_path = os.path.join(checkpoint_dir, 'pytorch_model.bin')\n",
        "\n",
        "            print(f\"üîÑ Processing Fold {fold}/{N_SPLITS} from: {checkpoint_dir}\")\n",
        "\n",
        "            model = InferenceModel(model_name=MODEL_NAME, num_labels=NUM_LABELS)\n",
        "\n",
        "            if weights_path.endswith('.safetensors'):\n",
        "                state_dict = load_file(weights_path, device=DEVICE.type)\n",
        "            else: # for .bin files\n",
        "                state_dict = torch.load(weights_path, map_location=DEVICE.type)\n",
        "\n",
        "            model.load_state_dict(state_dict, strict=False)\n",
        "            model.to(DEVICE)\n",
        "            model.eval()\n",
        "\n",
        "            # Use a temporary Trainer for easy prediction\n",
        "            trainer = Trainer(model=model)\n",
        "            raw_predictions = trainer.predict(test_dataset)\n",
        "            all_logits.append(raw_predictions.predictions)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Could not process Fold {fold}. Error: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not all_logits:\n",
        "        print(\"‚ùå No models were successfully loaded. Aborting evaluation.\")\n",
        "        return\n",
        "\n",
        "    # 5. Average the Logits\n",
        "    print(\"\\n--- 5. Averaging predictions (ensembling) ---\")\n",
        "    ensembled_logits = np.mean(all_logits, axis=0)\n",
        "    print(f\"‚úÖ Successfully ensembled predictions from {len(all_logits)} models.\")\n",
        "\n",
        "    # 6. Post-process Ensembled Predictions\n",
        "    print(\"\\n--- 6. Applying adaptive thresholding to ensembled predictions ---\")\n",
        "    print(f\"Using base_threshold: {OPTIMIZED_PARAMS['base_threshold']:.4f}\")\n",
        "    predicted_labels_list = adaptive_threshold_prediction(\n",
        "        ensembled_logits, all_labels, cooccurrence_prob, base_threshold=OPTIMIZED_PARAMS['base_threshold']\n",
        "    )\n",
        "\n",
        "    # 7. Evaluate Final Predictions on the Test Set\n",
        "    print(\"\\n--- 7. Final Evaluation on the Test Set ---\")\n",
        "    y_true_binary = mlb.transform(process_label_strings(test_df['labels_str']))\n",
        "    y_pred_binary = mlb.transform(predicted_labels_list)\n",
        "\n",
        "    weighted_f1 = f1_score(y_true_binary, y_pred_binary, average='weighted', zero_division=0)\n",
        "    jaccard = jaccard_score(y_true_binary, y_pred_binary, average='weighted', zero_division=0)\n",
        "\n",
        "    print(\"\\n--- ü•Å Final Ensembled Test Set Results ü•Å ---\")\n",
        "    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "    print(f\"Jaccard Score:     {jaccard:.4f}\")\n",
        "    print(\"------------------------------------\\n\")\n",
        "    print(\"--- Per-Label Performance (Test Set) ---\")\n",
        "    print(classification_report(y_true_binary, y_pred_binary, target_names=all_labels, zero_division=0))\n",
        "\n",
        "    # 8. Save Predictions to File\n",
        "    test_df['Predicted_Labels'] = [\",\".join(p) for p in predicted_labels_list]\n",
        "    prediction_output_path = os.path.join(RESULTS_DIR, \"arabert_optimized_kfold_predictions.tsv\")\n",
        "    test_df[['Predicted_Labels']].to_csv(prediction_output_path, sep='\\t', header=False, index=False)\n",
        "    print(f\"üíæ Test set predictions saved to: {prediction_output_path}\")\n",
        "    print(\"\\n‚úÖ Evaluation complete.\")\n",
        "\n",
        "\n",
        "# =================================================================================\n",
        "# Cell 6: Script Execution\n",
        "# =================================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Execute the training and evaluation workflows sequentially\n",
        "    main_training()\n",
        "    evaluate_kfold_ensemble()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YOT2rLt4Yxa3",
        "outputId": "3bfd9941-e37b-48ee-c6df-6fc5bc10abb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using model: aubmindlab/bert-base-arabertv2\n",
            "Using device: cuda\n",
            "\n",
            "--- Using Optimized Hyperparameters ---\n",
            "learning_rate: 5.273957732715589e-05\n",
            "num_train_epochs: 13\n",
            "weight_decay: 0.04131058607286182\n",
            "focal_alpha: 0.9702303056621574\n",
            "focal_gamma: 1.39543909126709\n",
            "base_threshold: 0.20408644287720523\n",
            "-------------------------------------\n",
            "\n",
            "========================================\n",
            "         STARTING TRAINING RUN          \n",
            "========================================\n",
            "\n",
            "--- Loading Data from Google Drive ---\n",
            "\n",
            "--- Preprocessing Labels ---\n",
            "Discovered 7 unique labels: ['A', 'B', 'C', 'D', 'E', 'F', 'Z']\n",
            "\n",
            "===== Fold 1/5 =====\n",
            "Training on 280 samples, Validating on 70 samples.\n",
            "Found 15 strong label co-occurrence patterns for this fold.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Initializing New Model for Fold ---\n",
            "\n",
            "--- Starting Fine-Tuning for Fold 1 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfatemah2024\u001b[0m (\u001b[33mfatemah2024-cu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250810_143223-nzprpppp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fatemah2024-cu/huggingface/runs/nzprpppp' target=\"_blank\">lunar-cherry-50</a></strong> to <a href='https://wandb.ai/fatemah2024-cu/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/fatemah2024-cu/huggingface' target=\"_blank\">https://wandb.ai/fatemah2024-cu/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/fatemah2024-cu/huggingface/runs/nzprpppp' target=\"_blank\">https://wandb.ai/fatemah2024-cu/huggingface/runs/nzprpppp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='455' max='455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [455/455 01:39, Epoch 13/13]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.179700</td>\n",
              "      <td>0.165880</td>\n",
              "      <td>0.563988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.152800</td>\n",
              "      <td>0.181379</td>\n",
              "      <td>0.596849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.130900</td>\n",
              "      <td>0.166508</td>\n",
              "      <td>0.567978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.106300</td>\n",
              "      <td>0.174637</td>\n",
              "      <td>0.582102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.071600</td>\n",
              "      <td>0.175359</td>\n",
              "      <td>0.573212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.046100</td>\n",
              "      <td>0.201336</td>\n",
              "      <td>0.600261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.029700</td>\n",
              "      <td>0.201153</td>\n",
              "      <td>0.577590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.020600</td>\n",
              "      <td>0.221713</td>\n",
              "      <td>0.584557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.015900</td>\n",
              "      <td>0.234655</td>\n",
              "      <td>0.583344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.012700</td>\n",
              "      <td>0.241008</td>\n",
              "      <td>0.583662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.011100</td>\n",
              "      <td>0.247877</td>\n",
              "      <td>0.583161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.010200</td>\n",
              "      <td>0.249452</td>\n",
              "      <td>0.575661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.009500</td>\n",
              "      <td>0.251954</td>\n",
              "      <td>0.578281</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Predictions on Validation Set for Fold ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 2/5 =====\n",
            "Training on 280 samples, Validating on 70 samples.\n",
            "Found 15 strong label co-occurrence patterns for this fold.\n",
            "\n",
            "--- Initializing New Model for Fold ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Fine-Tuning for Fold 2 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='455' max='455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [455/455 01:34, Epoch 13/13]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.176200</td>\n",
              "      <td>0.189139</td>\n",
              "      <td>0.569446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.143400</td>\n",
              "      <td>0.226553</td>\n",
              "      <td>0.566906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.152900</td>\n",
              "      <td>0.193434</td>\n",
              "      <td>0.574701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.162900</td>\n",
              "      <td>0.184656</td>\n",
              "      <td>0.574701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.157300</td>\n",
              "      <td>0.180070</td>\n",
              "      <td>0.574701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.157800</td>\n",
              "      <td>0.180233</td>\n",
              "      <td>0.574701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.156100</td>\n",
              "      <td>0.188971</td>\n",
              "      <td>0.574701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.182653</td>\n",
              "      <td>0.574701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.156000</td>\n",
              "      <td>0.181077</td>\n",
              "      <td>0.574701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.156100</td>\n",
              "      <td>0.178716</td>\n",
              "      <td>0.574701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.155100</td>\n",
              "      <td>0.179607</td>\n",
              "      <td>0.574701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.179794</td>\n",
              "      <td>0.574701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.154700</td>\n",
              "      <td>0.179684</td>\n",
              "      <td>0.574701</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Predictions on Validation Set for Fold ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 3/5 =====\n",
            "Training on 280 samples, Validating on 70 samples.\n",
            "Found 14 strong label co-occurrence patterns for this fold.\n",
            "\n",
            "--- Initializing New Model for Fold ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Fine-Tuning for Fold 3 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='455' max='455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [455/455 01:36, Epoch 13/13]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.182800</td>\n",
              "      <td>0.150918</td>\n",
              "      <td>0.621057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>0.145983</td>\n",
              "      <td>0.619085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.135900</td>\n",
              "      <td>0.141810</td>\n",
              "      <td>0.619438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>0.135132</td>\n",
              "      <td>0.630257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.073300</td>\n",
              "      <td>0.151303</td>\n",
              "      <td>0.639242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.049800</td>\n",
              "      <td>0.155104</td>\n",
              "      <td>0.650258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.034500</td>\n",
              "      <td>0.165301</td>\n",
              "      <td>0.651549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.023000</td>\n",
              "      <td>0.168704</td>\n",
              "      <td>0.651429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.017700</td>\n",
              "      <td>0.180912</td>\n",
              "      <td>0.651013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>0.183129</td>\n",
              "      <td>0.654145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.012700</td>\n",
              "      <td>0.187322</td>\n",
              "      <td>0.663900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.011300</td>\n",
              "      <td>0.186008</td>\n",
              "      <td>0.666794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>0.187265</td>\n",
              "      <td>0.664111</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Predictions on Validation Set for Fold ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 4/5 =====\n",
            "Training on 280 samples, Validating on 70 samples.\n",
            "Found 13 strong label co-occurrence patterns for this fold.\n",
            "\n",
            "--- Initializing New Model for Fold ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Fine-Tuning for Fold 4 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='455' max='455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [455/455 01:46, Epoch 13/13]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.183300</td>\n",
              "      <td>0.144685</td>\n",
              "      <td>0.612201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.154800</td>\n",
              "      <td>0.151199</td>\n",
              "      <td>0.606231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.135700</td>\n",
              "      <td>0.157604</td>\n",
              "      <td>0.634713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.102500</td>\n",
              "      <td>0.166801</td>\n",
              "      <td>0.625050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.077300</td>\n",
              "      <td>0.159128</td>\n",
              "      <td>0.614970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.053600</td>\n",
              "      <td>0.176593</td>\n",
              "      <td>0.637347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.037400</td>\n",
              "      <td>0.187295</td>\n",
              "      <td>0.645215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.026000</td>\n",
              "      <td>0.195009</td>\n",
              "      <td>0.626399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.020900</td>\n",
              "      <td>0.211595</td>\n",
              "      <td>0.616186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.017200</td>\n",
              "      <td>0.206913</td>\n",
              "      <td>0.633467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.013800</td>\n",
              "      <td>0.208605</td>\n",
              "      <td>0.634004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.214205</td>\n",
              "      <td>0.630428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.011500</td>\n",
              "      <td>0.214024</td>\n",
              "      <td>0.643648</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Predictions on Validation Set for Fold ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 5/5 =====\n",
            "Training on 280 samples, Validating on 70 samples.\n",
            "Found 13 strong label co-occurrence patterns for this fold.\n",
            "\n",
            "--- Initializing New Model for Fold ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Fine-Tuning for Fold 5 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='455' max='455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [455/455 01:40, Epoch 13/13]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.181800</td>\n",
              "      <td>0.177203</td>\n",
              "      <td>0.575789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.162900</td>\n",
              "      <td>0.160900</td>\n",
              "      <td>0.569113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.168300</td>\n",
              "      <td>0.156280</td>\n",
              "      <td>0.569113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.164300</td>\n",
              "      <td>0.159914</td>\n",
              "      <td>0.569113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.162400</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.569113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.161300</td>\n",
              "      <td>0.159288</td>\n",
              "      <td>0.569113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.160500</td>\n",
              "      <td>0.161930</td>\n",
              "      <td>0.569113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.160900</td>\n",
              "      <td>0.156312</td>\n",
              "      <td>0.569113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.162000</td>\n",
              "      <td>0.161477</td>\n",
              "      <td>0.569113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.159900</td>\n",
              "      <td>0.159214</td>\n",
              "      <td>0.569113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.160300</td>\n",
              "      <td>0.160456</td>\n",
              "      <td>0.569113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.159900</td>\n",
              "      <td>0.159339</td>\n",
              "      <td>0.569113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.159400</td>\n",
              "      <td>0.159620</td>\n",
              "      <td>0.569113</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Predictions on Validation Set for Fold ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "===== Overall K-Fold Performance Analysis (OOF) =====\n",
            "\n",
            "Overall Weighted F1 Score across all folds: 0.6066\n",
            "\n",
            "--- Overall Per-Label Performance (based on OOF predictions) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.59      0.99      0.74       197\n",
            "           B       0.60      0.98      0.74       203\n",
            "           C       0.16      0.23      0.19        22\n",
            "           D       0.25      0.79      0.38        80\n",
            "           E       0.29      0.91      0.43        87\n",
            "           F       0.00      0.00      0.00        14\n",
            "           Z       0.00      0.00      0.00         6\n",
            "\n",
            "   micro avg       0.44      0.89      0.59       609\n",
            "   macro avg       0.27      0.56      0.36       609\n",
            "weighted avg       0.47      0.89      0.61       609\n",
            " samples avg       0.46      0.92      0.58       609\n",
            "\n",
            "‚úÖ Training complete.\n",
            "\n",
            "\n",
            "========================================\n",
            "        STARTING EVALUATION RUN         \n",
            "========================================\n",
            "\n",
            "--- 1. Loading tokenizer and datasets ---\n",
            "\n",
            "--- 2. Preprocessing labels for evaluation context ---\n",
            "Built co-occurrence map from 350 training samples.\n",
            "\n",
            "--- 3. Tokenizing the test set ---\n",
            "\n",
            "--- 4. Generating predictions from 5 ensembled models ---\n",
            "üîÑ Processing Fold 1/5 from: /content/drive/MyDrive/AraHealthQA/MentalQA/Task1/output/arabert_optimized_kfold_fold_1/checkpoint-210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Processing Fold 2/5 from: /content/drive/MyDrive/AraHealthQA/MentalQA/Task1/output/arabert_optimized_kfold_fold_2/checkpoint-105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Processing Fold 3/5 from: /content/drive/MyDrive/AraHealthQA/MentalQA/Task1/output/arabert_optimized_kfold_fold_3/checkpoint-420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Processing Fold 4/5 from: /content/drive/MyDrive/AraHealthQA/MentalQA/Task1/output/arabert_optimized_kfold_fold_4/checkpoint-245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Processing Fold 5/5 from: /content/drive/MyDrive/AraHealthQA/MentalQA/Task1/output/arabert_optimized_kfold_fold_5/checkpoint-35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 5. Averaging predictions (ensembling) ---\n",
            "‚úÖ Successfully ensembled predictions from 5 models.\n",
            "\n",
            "--- 6. Applying adaptive thresholding to ensembled predictions ---\n",
            "Using base_threshold: 0.2041\n",
            "\n",
            "--- 7. Final Evaluation on the Test Set ---\n",
            "\n",
            "--- ü•Å Final Ensembled Test Set Results ü•Å ---\n",
            "Weighted F1 Score: 0.2597\n",
            "Jaccard Score:     0.1940\n",
            "------------------------------------\n",
            "\n",
            "--- Per-Label Performance (Test Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.57      0.94      0.71        84\n",
            "           B       0.00      0.00      0.00        85\n",
            "           C       0.07      1.00      0.12        10\n",
            "           D       0.18      0.18      0.18        34\n",
            "           E       0.00      0.00      0.00        38\n",
            "           F       0.05      1.00      0.09         6\n",
            "           Z       0.02      1.00      0.04         3\n",
            "\n",
            "   micro avg       0.17      0.40      0.24       260\n",
            "   macro avg       0.13      0.59      0.16       260\n",
            "weighted avg       0.21      0.40      0.26       260\n",
            " samples avg       0.17      0.37      0.23       260\n",
            "\n",
            "üíæ Test set predictions saved to: /content/drive/MyDrive/AraHealthQA/MentalQA/Task1/results/arabert_optimized_kfold_predictions.tsv\n",
            "\n",
            "‚úÖ Evaluation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-DvfMYhugAKV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}